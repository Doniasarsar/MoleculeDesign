{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YV5Ko7mX9aC"
      },
      "source": [
        "# **Business Understanding**\n",
        "The goal of this project is to optimize the process of drug discovery by automatise the lead compound step which decrease the time and the cost of a drug production. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksylEiv2YDdV"
      },
      "source": [
        "# **Defining the data requirements:**\n",
        "We need a dataset of small molecules and their properties.\n",
        "\n",
        "###  Identify potential data sources:\n",
        "We have 2 data sources : Chembl - Pubchem "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H1JoVY0YN0V"
      },
      "source": [
        "# **Data Understanding & Data Preparation**\n",
        "\n",
        "### 1. Data Understanding:\n",
        "- Explore the data and get a feel for what it contains.\n",
        "- Check the shape of dataset.\n",
        "- Check data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CO_LmY1eSDdN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme exï¿½cutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zkSCOCUfSWEl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPNYG9tWNHva"
      },
      "source": [
        "# **Chembl dataset analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-QYLvizmpxnl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas-profiling\n",
            "  Using cached pandas_profiling-3.6.6-py2.py3-none-any.whl (324 kB)\n",
            "Collecting ydata-profiling (from pandas-profiling)\n",
            "  Using cached ydata_profiling-4.1.2-py2.py3-none-any.whl (345 kB)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 23.1.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: scipy<1.10,>=1.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (1.7.1)\n",
            "Requirement already satisfied: pandas!=1.4.0,<1.6,>1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (1.3.4)\n",
            "Requirement already satisfied: matplotlib<3.7,>=3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (3.4.3)\n",
            "Collecting pydantic<1.11,>=1.8.1 (from ydata-profiling->pandas-profiling)\n",
            "  Using cached pydantic-1.10.7-cp39-cp39-win_amd64.whl (2.2 MB)\n",
            "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (6.0)\n",
            "Requirement already satisfied: jinja2<3.2,>=2.11.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (2.11.3)\n",
            "Collecting visions[type_image_path]==0.7.5 (from ydata-profiling->pandas-profiling)\n",
            "  Using cached visions-0.7.5-py3-none-any.whl (102 kB)\n",
            "Requirement already satisfied: numpy<1.24,>=1.16.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (1.22.4)\n",
            "Requirement already satisfied: htmlmin==0.1.12 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (0.1.12)\n",
            "Collecting phik<0.13,>=0.11.1 (from ydata-profiling->pandas-profiling)\n",
            "  Using cached phik-0.12.3-cp39-cp39-win_amd64.whl (663 kB)\n",
            "Requirement already satisfied: requests<2.29,>=2.24.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (2.26.0)\n",
            "Requirement already satisfied: tqdm<4.65,>=4.48.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (4.62.3)\n",
            "Requirement already satisfied: seaborn<0.13,>=0.10.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (0.11.2)\n",
            "Collecting multimethod<1.10,>=1.4 (from ydata-profiling->pandas-profiling)\n",
            "  Using cached multimethod-1.9.1-py3-none-any.whl (10 kB)\n",
            "Collecting statsmodels<0.14,>=0.13.2 (from ydata-profiling->pandas-profiling)\n",
            "  Using cached statsmodels-0.13.5-cp39-cp39-win_amd64.whl (9.2 MB)\n",
            "Requirement already satisfied: typeguard<2.14,>=2.13.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (2.13.3)\n",
            "Collecting imagehash==4.3.1 (from ydata-profiling->pandas-profiling)\n",
            "  Using cached ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "Requirement already satisfied: PyWavelets in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (1.1.1)\n",
            "Requirement already satisfied: pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (8.4.0)\n",
            "Requirement already satisfied: attrs>=19.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (21.2.0)\n",
            "Requirement already satisfied: networkx>=2.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (2.6.3)\n",
            "Requirement already satisfied: tangled-up-in-unicode>=0.0.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling->pandas-profiling) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (1.3.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas!=1.4.0,<1.6,>1.1->ydata-profiling->pandas-profiling) (2021.3)\n",
            "Requirement already satisfied: joblib>=0.14.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from phik<0.13,>=0.11.1->ydata-profiling->pandas-profiling) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<1.11,>=1.8.1->ydata-profiling->pandas-profiling) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (3.2)\n",
            "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from statsmodels<0.14,>=0.13.2->ydata-profiling->pandas-profiling) (0.5.2)\n",
            "Requirement already satisfied: packaging>=21.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from statsmodels<0.14,>=0.13.2->ydata-profiling->pandas-profiling) (23.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<4.65,>=4.48.2->ydata-profiling->pandas-profiling) (0.4.4)\n",
            "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (1.16.0)\n",
            "Installing collected packages: pydantic, multimethod, imagehash, visions, statsmodels, phik, ydata-profiling, pandas-profiling\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.12.2\n",
            "    Uninstalling statsmodels-0.12.2:\n",
            "      Successfully uninstalled statsmodels-0.12.2\n",
            "Successfully installed imagehash-4.3.1 multimethod-1.9.1 pandas-profiling-3.6.6 phik-0.12.3 pydantic-1.10.7 statsmodels-0.13.5 visions-0.7.5 ydata-profiling-4.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas-profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "10_i3nmcmg4e"
      },
      "outputs": [],
      "source": [
        "#importation of required librairies\n",
        "import pandas as pd \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KQr1So9_qkEc"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas_profiling'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18064/2274191625.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_profiling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
          ]
        }
      ],
      "source": [
        "from pandas_profiling import ProfileReport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwjwW0gMPkL9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpnmdthSBqbU"
      },
      "outputs": [],
      "source": [
        "#Chembl = pd.read_csv(r'/content/Chembl.csv', sep=';', error_bad_lines =False, engine='python')\n",
        "Chembl = pd.read_csv('/content/drive/MyDrive/Chembl.csv',sep=';', error_bad_lines =False, engine='python')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-Y5i2Y1Xsh1"
      },
      "outputs": [],
      "source": [
        "Chembl.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvzeH027Q5bf"
      },
      "source": [
        "Work on a fixed subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXsDGT_rQ2ee"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Set the size of the subset (e.g. 10,000)\n",
        "subset_size = 10000\n",
        "\n",
        "# Randomly select a fixed subset of your data\n",
        "subset = Chembl.sample(n=subset_size, random_state=123)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBXDjHiHqTl_"
      },
      "outputs": [],
      "source": [
        "profil=ProfileReport(subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7nOYU2FWbcC"
      },
      "outputs": [],
      "source": [
        "profil.to_file(\"Analysis.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKo7woHnYjV1"
      },
      "source": [
        "# **Data preprocessing**\n",
        "\n",
        "\n",
        "1.   Delete unnecessary columns.\n",
        "2.   Delete molecules with 1050 < weight < 50.\n",
        "3.   Convert None to  Nan. \n",
        "4.   Convert categorical type for some columns.\n",
        "5.   Impute nan values\n",
        "6.   Standardization. \n",
        "7.   Calcul descriptors.\n",
        "8.   Features selection.\n",
        "9.   Normalization. \n",
        "10.  Plots.\n",
        "11.  Visualise some mols in 2D or 3D.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30HA__ErppPj"
      },
      "outputs": [],
      "source": [
        "subset.head(100)\n",
        "subset[subset['Molecular Species'] == 'ZWITTERION'].head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwqEUC2geZZd"
      },
      "outputs": [],
      "source": [
        "#Chem_data= subset.copy()\n",
        "Chem_data= Chembl.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKUgHtej43ZQ"
      },
      "source": [
        "### 1. Delete unnecessary columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FDwphtZ6Nuz"
      },
      "source": [
        "ajouter des expliquations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jjSzK2Re2Bi"
      },
      "outputs": [],
      "source": [
        "Chem_data.drop(['ChEMBL ID',\t'Name',\t'Synonyms',\t'Type',\t'Max Phase','Targets','Structure Type',], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GJWTPYsEB0q"
      },
      "outputs": [],
      "source": [
        "Chem_data.drop(['Inchi Key',], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cuf21n4lfo6P"
      },
      "outputs": [],
      "source": [
        "Chem_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y1JUZ3Q5FqA"
      },
      "outputs": [],
      "source": [
        "Chem_data.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWEg85F05TtW"
      },
      "source": [
        "## 2. Delete molecules with 1050 < weight < 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UhL6KxIjkWu"
      },
      "outputs": [],
      "source": [
        "Chem_data['Molecular Weight'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gRe0fLFgRWU"
      },
      "outputs": [],
      "source": [
        "over_weightedd = Chem_data[(Chem_data['Molecular Weight'] >= 1050.0) | (Chem_data['Molecular Weight'] <= 50.0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zy4xZMHjU-V"
      },
      "outputs": [],
      "source": [
        "over_weightedd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxHBkn8Qlpvw"
      },
      "outputs": [],
      "source": [
        "over_weighted = Chem_data[(Chem_data['Molecular Weight'] >= 1050.0) | (Chem_data['Molecular Weight'] <= 50.0)].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-ZNMCUsibEd"
      },
      "outputs": [],
      "source": [
        "Chem_data.drop( over_weighted , inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOBx0QZRlzYw"
      },
      "outputs": [],
      "source": [
        "Chem_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKMvmhf_5T2R"
      },
      "source": [
        "## 3. Convert None to  Nan "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w2tdLobeyeI"
      },
      "outputs": [],
      "source": [
        "Chem_data.replace('None',np.nan, inplace=True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO5TJVXc5TxP"
      },
      "source": [
        "## 4. Convert categorical type for some columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOEfE6AjoCO0"
      },
      "outputs": [],
      "source": [
        "Chem_data[['AlogP','Polar Surface Area','QED Weighted','CX Acidic pKa','CX Basic pKa','CX LogP','CX LogD','Molecular Weight (Monoisotopic)']] = Chem_data[['AlogP','Polar Surface Area','QED Weighted','CX Acidic pKa','CX Basic pKa','CX LogP','CX LogD','Molecular Weight (Monoisotopic)']].astype(float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tNaSCyztMCz"
      },
      "outputs": [],
      "source": [
        "# we can't convert this columns to int because we have nan so we are going to convert them to float, handel the nan values and then convert them to in  \n",
        "Chem_data[['HBA','HBD','#RO5 Violations','#Rotatable Bonds','Aromatic Rings','Inorganic Flag','Heavy Atoms','HBA (Lipinski)','HBD (Lipinski)','#RO5 Violations (Lipinski)']] = Chem_data[['HBA','HBD','#RO5 Violations','#Rotatable Bonds','Aromatic Rings','Inorganic Flag','Heavy Atoms','HBA (Lipinski)','HBD (Lipinski)','#RO5 Violations (Lipinski)']].astype(float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1ag8WYpu3be"
      },
      "outputs": [],
      "source": [
        "Chem_data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyiUTekF5Tcw"
      },
      "source": [
        "## 5. Impute Nan values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrYWclQryY8H"
      },
      "outputs": [],
      "source": [
        "#Resume table of columns with missing values\n",
        "def missing_values_table(Chem_data):\n",
        "    mis_val = Chem_data.isnull().sum()\n",
        "    mis_val_percent = 100 * Chem_data.isnull().sum() / len(Chem_data)\n",
        "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "    mis_val_table_ren_columns = mis_val_table.rename(\n",
        "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "        '% of Total Values', ascending=False).round(1)\n",
        "    print (\"Your selected dataframe has \" + str(Chem_data.shape[1]) + \" columns.\\n\"\n",
        "                                                               \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
        "           \" columns that have missing values.\")\n",
        "    return mis_val_table_ren_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMvoMSMdyZzQ"
      },
      "outputs": [],
      "source": [
        "missing_values_table(Chem_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pujyf56Xy_NF"
      },
      "outputs": [],
      "source": [
        "# Get the columns with > 50% missing\n",
        "na_threshold = 50\n",
        "missing_df = missing_values_table(Chem_data);\n",
        "missing_columns = list(missing_df[missing_df['% of Total Values'] > na_threshold].index)\n",
        "print(f'We will remove {len(missing_columns)} columns as we set threshold of 50%.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMpeAv_A5TUq"
      },
      "source": [
        "### 5.1 Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckZkEkC80dwP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5vZ45fl4tT9"
      },
      "outputs": [],
      "source": [
        "num_cols_ab = Chem_data.select_dtypes(\"float64\")\n",
        "num_cols = Chem_data.select_dtypes(include=[np.number]).columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMWemoee_mhT"
      },
      "outputs": [],
      "source": [
        "num_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOeuGyMrz5tO"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(ncols=5, nrows=5, figsize=(16, 20), squeeze=False)\n",
        "plt.subplots_adjust(wspace=.4)\n",
        "\n",
        "sampled_data = Chem_data.sample(n=10000) # randomly sample 1000 rows from the dataset\n",
        "\n",
        "for ax, col in zip(axs.ravel(), Chem_data.columns):\n",
        "    ax.hist(sampled_data[col].dropna())\n",
        "    ax.title.set_text(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcEHCLqN5TNy"
      },
      "source": [
        "### 5.2 outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPXG_bwF4ZSX"
      },
      "outputs": [],
      "source": [
        "# valeur abberantes \n",
        "fig, axs = plt.subplots(ncols=5, nrows=2, figsize=(16, 20), squeeze=False)\n",
        "plt.subplots_adjust(wspace=.4)\n",
        "\n",
        "for ax, col in zip(axs.ravel(), num_cols_ab.columns):\n",
        "    ax.boxplot(Chem_data[col].dropna())\n",
        "    ax.title.set_text(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoTev0SQ-4fy"
      },
      "source": [
        "* Our data is not normally distributed and there are many outliers so the method of imputation to use is the median "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwAZatDaAjrg"
      },
      "outputs": [],
      "source": [
        "median_val = Chem_data[num_cols].median()\n",
        "median_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbmiAuJPBo4z"
      },
      "outputs": [],
      "source": [
        "Chem_data[num_cols] = Chem_data[num_cols].fillna(median_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWC0ODtEBsZA"
      },
      "outputs": [],
      "source": [
        "missing_values_table(Chem_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fngqCJizDZZK"
      },
      "source": [
        "* for these categorical data it does'nt make sense to impute them cuz the values are unique for each molecule so we gonna drop them "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K6qh0-GDr5B"
      },
      "outputs": [],
      "source": [
        "Chem_data.dropna(axis=0, inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0stnOeSEVFw"
      },
      "outputs": [],
      "source": [
        "Chem_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6hj277wFH6l"
      },
      "outputs": [],
      "source": [
        "missing_values_table(Chem_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBq8fGhCFUOO"
      },
      "source": [
        "Check if there is any duplicated rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V551Igq7F2U4"
      },
      "outputs": [],
      "source": [
        "Chem_data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTiIg6RKFzaC"
      },
      "outputs": [],
      "source": [
        "Chem_data.drop_duplicates(keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob0sIPI1HClo"
      },
      "outputs": [],
      "source": [
        "# reconvert these columns to int\n",
        "Chem_data[['HBA','HBD','#RO5 Violations','#Rotatable Bonds','Aromatic Rings','Inorganic Flag','Heavy Atoms','HBA (Lipinski)','HBD (Lipinski)','#RO5 Violations (Lipinski)']] = Chem_data[['HBA','HBD','#RO5 Violations','#Rotatable Bonds','Aromatic Rings','Inorganic Flag','Heavy Atoms','HBA (Lipinski)','HBD (Lipinski)','#RO5 Violations (Lipinski)']].astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8POAiIP21Bfb"
      },
      "outputs": [],
      "source": [
        "int_cols=Chem_data[['HBA','HBD','#RO5 Violations','#Rotatable Bonds','Aromatic Rings','Inorganic Flag','Heavy Atoms','HBA (Lipinski)','HBD (Lipinski)','#RO5 Violations (Lipinski)']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5VhE9Ps1IXg"
      },
      "outputs": [],
      "source": [
        "int_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1LEENIn14aH"
      },
      "outputs": [],
      "source": [
        "num_cols = list(set(num_cols).difference(set(int_cols)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6J8FbUu15zZ"
      },
      "outputs": [],
      "source": [
        "num_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSL3e16KHJ9g"
      },
      "outputs": [],
      "source": [
        "Chem_data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjmtlbzw_BRW"
      },
      "source": [
        "## . Calcul Topological Descriptor : to remove the data we have is already descriptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spO777XdU_e2"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP1J1D49-_lm"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pZEyWPOYMdq"
      },
      "outputs": [],
      "source": [
        "import rdkit.Chem.Descriptors as Descriptors\n",
        "\n",
        "print(dir(Descriptors))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsArynrWNZSC"
      },
      "outputs": [],
      "source": [
        "# Define a function to calculate the descriptors for a given molecule\n",
        "def calculate_descriptors(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    mw = Descriptors.MolWt(mol)\n",
        "    logp = Descriptors.MolLogP(mol)\n",
        "    psa = Descriptors.TPSA(mol)\n",
        "    hba = Descriptors.NumHAcceptors(mol)\n",
        "    hbd = Descriptors.NumHDonors(mol)\n",
        "    ro5_violations = Descriptors.NumRotatableBonds(mol)\n",
        "    passes_ro3 = Descriptors.NumHeteroatoms(mol)\n",
        "    qed_weighted = Descriptors.qed(mol)\n",
        "   # cx_acidic_pka = Descriptors.PredictedLogAC(mol)\n",
        "   # cx_basic_pka = Descriptors.PredictedLogBC(mol)\n",
        "    cx_logp = Descriptors.MolLogP(mol)\n",
        "    cx_logd = Descriptors.MolLogP(mol)\n",
        "    aromatic_rings = Descriptors.NumAromaticRings(mol)\n",
        "    inorganic_flag = 1 if any(atom.GetSymbol() not in ['C', 'H', 'O', 'N', 'S', 'P', 'F', 'Cl', 'Br', 'I'] for atom in mol.GetAtoms()) else 0\n",
        "    heavy_atoms = Descriptors.HeavyAtomCount(mol)\n",
        "    hba_lipinski = Descriptors.NumHAcceptors(mol)\n",
        "    hbd_lipinski = Descriptors.NumHDonors(mol)\n",
        "    ro5_violations_lipinski = 1 if (hba_lipinski + hbd_lipinski) > 5 or ro5_violations > 1 else 0\n",
        "    mw_monoisotopic = Descriptors.ExactMolWt(mol)\n",
        "    molecular_species = Chem.rdinchi.InchiToInchiKey(Chem.inchi.MolToInchi(mol))\n",
        "    #return [mw, logp, psa, hba, hbd, ro5_violations, passes_ro3, qed_weighted, cx_acidic_pka, cx_basic_pka, cx_logp, cx_logd, aromatic_rings, inorganic_flag, heavy_atoms, hba_lipinski, hbd_lipinski, ro5_violations_lipinski, mw_monoisotopic, molecular_species]\n",
        "    descriptors_dict = {'Molecular Weight': mw,\n",
        "                        'AlogP': logp,\n",
        "                        'Polar Surface Area': psa,\n",
        "                        'HBA': hba,\n",
        "                        'HBD': hbd,\n",
        "                        '#RO5 Violations': ro5_violations,\n",
        "                        'Passes Ro3':  passes_ro3,\n",
        "                        'QED Weighted': qed_weighted,\n",
        "                        #'CX Acidic pKa': cx_acidic_pka,\n",
        "                        #'CX Basic pKa': cx_basic_pka,\n",
        "                        'CX LogP': cx_logp,\n",
        "                        'CX LogD': cx_logd,\n",
        "                        'Aromatic Rings': aromatic_rings,\n",
        "                        'Inorganic Flag': inorganic_flag,\n",
        "                        'Heavy Atoms': heavy_atoms,\n",
        "                        'HBA (Lipinski)': hba_lipinski,\n",
        "                        'HBD (Lipinski)': hbd_lipinski,\n",
        "                        '#RO5 Violations (Lipinski)': ro5_violations_lipinski,\n",
        "                        'Molecular Weight (Monoisotopic)': mw_monoisotopic,\n",
        "                        'Molecular Species': molecular_species\n",
        "                        }\n",
        "\n",
        "    return descriptors_dict\n",
        "    #return [mw, logp, psa, hba, hbd, ro5_violations, passes_ro3, qed_weighted, cx_logp, cx_logd, aromatic_rings, inorganic_flag, heavy_atoms, hba_lipinski, hbd_lipinski, ro5_violations_lipinski, mw_monoisotopic, molecular_species, molecular_formula]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3y-O3FfNoCb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a new dataframe to store the descriptor values for each molecule\n",
        "descriptors = pd.DataFrame(columns=['Smiles', 'Molecular Weight', 'AlogP', 'Polar Surface Area', 'HBA', 'HBD', '#RO5 Violations', '#Rotatable Bonds', 'Passes Ro3', 'QED Weighted', 'CX Acidic pKa', 'CX Basic pKa', 'CX LogP', 'CX LogD', 'Aromatic Rings', 'Inorganic Flag', 'Heavy Atoms', 'HBA (Lipinski)', 'HBD (Lipinski)', '#RO5 Violations (Lipinski)', 'Molecular Weight (Monoisotopic)', 'Molecular Species', 'Molecular Formula'])\n",
        "\n",
        "# Loop through each SMILES string in your dataset and calculate the descriptors for each molecule\n",
        "for smiles in Chem_data['Smiles']:\n",
        "    descriptors_row = calculate_descriptors(smiles)\n",
        "    descriptors_row['Smiles'] = smiles\n",
        "    descriptors = descriptors.append(descriptors_row, ignore_index=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do5l6QoHVlYR"
      },
      "outputs": [],
      "source": [
        "descriptors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azq1LfpTPdgT"
      },
      "source": [
        "## Encoding \n",
        "\n",
        "1.   Passes Ro3 encoding : '0': N ,'1' : Y\n",
        "2.   Molecular Species encoding : '0':ACID , '1':BASE ,'2':NEUTRAL, '3':ZWITTERION  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFQbwW1DoBkS"
      },
      "outputs": [],
      "source": [
        "Chem_data['Passes Ro3'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsr9zV43PUcU"
      },
      "outputs": [],
      "source": [
        "subset['Molecular Species'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIOckuc7oLD5"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa0vz29boU_Z"
      },
      "outputs": [],
      "source": [
        "# create a label encoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# fit the encoder to the categories and transform them into integers\n",
        "Chem_data['Passes Ro3'] = label_encoder.fit_transform(Chem_data['Passes Ro3'])\n",
        "\n",
        "\n",
        "#Chem_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u4hVPcApJU5"
      },
      "outputs": [],
      "source": [
        "Chem_data['Passes Ro3'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vt2qb1YpKqR"
      },
      "outputs": [],
      "source": [
        "# create a label encoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# fit the encoder to the categories and transform them into integers\n",
        "Chem_data['Molecular Species'] = label_encoder.fit_transform(Chem_data['Molecular Species'])\n",
        "\n",
        "\n",
        "#Chem_data.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGKgSLB9rTMy"
      },
      "outputs": [],
      "source": [
        "row_index = 538971\n",
        "row = Chem_data.loc[row_index]\n",
        "row\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr8_huq8G2c-"
      },
      "source": [
        "## . Feature Selection \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Wo6eFyPHF_"
      },
      "source": [
        "* Correlation Matrix and ClusterMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFIBA5Z1tf8b"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AMgqLWgtQBr"
      },
      "outputs": [],
      "source": [
        "corr_matrix = Chem_data.select_dtypes(float).corr()\n",
        "#corr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKnqkFW7uAb2"
      },
      "outputs": [],
      "source": [
        "#plt.figure(figsize=(5500, 5000))\n",
        "#sns.clustermap(cor_mat, colors_ratio=0.01, annot=True)\n",
        "\n",
        "sns.clustermap(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1, figsize=(20, 10),annot=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_jF1OhOzUi_"
      },
      "outputs": [],
      "source": [
        "# extract highly correlated features\n",
        "threshold = 0.8 # set a correlation threshold\n",
        "corr_threshold = np.where(abs(corr_matrix) > threshold)\n",
        "corr_pairs = [(corr_matrix.iloc[corr_threshold[0][i], corr_threshold[1][i]], \n",
        "               corr_matrix.columns[corr_threshold[1][i]], \n",
        "               corr_matrix.columns[corr_threshold[0][i]]) for i in range(len(corr_threshold[0])) if corr_threshold[0][i] < corr_threshold[1][i]]\n",
        "\n",
        "# print highly correlated features\n",
        "for corr in corr_pairs:\n",
        "    print(corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgX-SEZ48up7"
      },
      "source": [
        "Ã  tester "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hxo6ft05CAL"
      },
      "outputs": [],
      "source": [
        "# case 1 : \n",
        "Chem_data.drop(['Heavy Atoms', 'Molecular Weight','AlogP','Polar Surface Area','HBA','HBD','#RO5 Violations','CX LogD',], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZNHtV298x0u"
      },
      "outputs": [],
      "source": [
        "Chem_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QHRJ-R-8DK_"
      },
      "outputs": [],
      "source": [
        "# case 2: \n",
        "#Chem_data.drop(['Heavy Atoms', 'Molecular Weight (Monoisotopic)','HBA (Lipinski)','HBD (Lipinski)','#RO5 Violations (Lipinski)','CX LogP'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FriazSCgwFo"
      },
      "outputs": [],
      "source": [
        "#save cleaned data to new file \n",
        "Chem_data.to_csv('cleaned_Chem_dataset_vers1.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTXCSJoNm3ON"
      },
      "source": [
        "## 7. Standardization : \n",
        "1. from the distribution plots we can see that our data is not normaly distributed so we need to standrize their values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYVv0AYym250"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75VnC-FMnYKA"
      },
      "outputs": [],
      "source": [
        "# Create the scaler object\n",
        "scaler = StandardScaler()\n",
        "num_cols_ab = Chem_data.select_dtypes(\"float64\").columns\n",
        "#num_cols = Chem_data.select_dtypes(include=[np.number]).columns\n",
        "# Fit and transform the data\n",
        "Chem_data[num_cols_ab] = scaler.fit_transform(Chem_data[num_cols_ab])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx9ClFtnz253"
      },
      "outputs": [],
      "source": [
        "Chem_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP2WqT4sYoWG"
      },
      "source": [
        "# **Data Modeling & Model Evaluation**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx2vMbpzTqLx"
      },
      "source": [
        "## Split data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVeUQ8vpTskD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0jIoSyqTwCi"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(Chem_data, test_size=0.3, random_state=45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMN1lJNRQJSb"
      },
      "outputs": [],
      "source": [
        "Chem_data.drop(['Molecular Formula'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUTj9jloNELp"
      },
      "outputs": [],
      "source": [
        "X_train, X_test,y_train, y_test = train_test_split(Chem_data.drop(['Smiles'],axis=1),Chem_data['Smiles'],test_size=0.3, random_state=45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2IEz69uZKrF"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OY2V7j9ZNjh"
      },
      "outputs": [],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml-hu8fVAUbG"
      },
      "source": [
        "## Nouvelle section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BctugyESZ0uZ"
      },
      "outputs": [],
      "source": [
        "!pip -q install rdkit-pypi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh_FClfObij6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzM_UJL5cSZU"
      },
      "outputs": [],
      "source": [
        "!pip install --pre deepchem\n",
        "import deepchem\n",
        "deepchem.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIEqy9aYYIhd"
      },
      "outputs": [],
      "source": [
        "#from rdkit import Chem\n",
        "#from rdkit.Chem import AllChem\n",
        "#from rdkit.Chem import DataStructs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LeakyReLU\n",
        "from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
        "#from deepchem.feat.smiles_tokenizer import DeepSMILESTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN3ib07vdOaD"
      },
      "outputs": [],
      "source": [
        "#tokenizer = BasicSmilesTokenizer()\n",
        "#print(tokenizer.tokenize(Chem_data['Smiles']))\n",
        "\n",
        "#from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
        "# Preprocess the SMILES strings using RDKit\n",
        "#tokenizer = SmilesTokenizer(Chem_data['Smiles'])\n",
        "#Chem_data['Smiles'] = Chem_data['Smiles'].apply(tokenizer.Tokenize)\n",
        "\n",
        "# Load the pretrained tokenizer\n",
        "#tokenizer = SmilesTokenizer.from_pretrained('smiles')\n",
        "\n",
        "# Define a function to tokenize a SMILES string\n",
        "#def tokenize(smiles):\n",
        " #   return tokenizer.tokenize(smiles)\n",
        "\n",
        "# Apply the tokenizer to the SMILES column\n",
        "#Chem_data['Smiles'] = Chem_data['Smiles'].apply(tokenize)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "smiles_series = Chem_data['Smiles']\n",
        "\n",
        "smiles_list = smiles_series.tolist()\n",
        "encoded_smiles = tokenizer(smiles_list, padding=True, truncation=True, max_length=100, return_tensors=\"pt\")\n",
        "\n",
        "encoded_smiles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuEB7eKHaVzS"
      },
      "outputs": [],
      "source": [
        "# Define the architecture of the GAN\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU\n",
        "\n",
        "#tokenize a SMILES string\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "smiles_series = Chem_data['Smiles']\n",
        "\n",
        "smiles_list = smiles_series.tolist()\n",
        "encoded_smiles = tokenizer(smiles_list, padding=True, truncation=True, max_length=100, return_tensors=\"pt\")\n",
        "\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "# Define the architecture of the GAN\n",
        "generator = Sequential()\n",
        "generator.add(Dense(128, input_dim=100))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(Dense(256))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(Dense(512))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Dense(512, input_dim=vocab_size))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dense(256))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCOA2wtdZvVc"
      },
      "outputs": [],
      "source": [
        "# Compile the GAN model\n",
        "gan_input = generator.input\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train the GAN on the training set\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the discriminator\n",
        "    real_samples = train_data.sample(batch_size)\n",
        "    real_X, real_y = real_samples[['property1', 'property2', 'property3']], np.ones((batch_size, 1))\n",
        "    fake_samples = generator.predict(np.random.normal(0, 1, (batch_size, 100)))\n",
        "    fake_X, fake_y = fake_samples, np.zeros((batch_size, 1))\n",
        "    X, y = np.vstack((real_X, fake_X)), np.vstack((real_y, fake_y))\n",
        "    discriminator.trainable = True\n",
        "    discriminator.train_on_batch(X, y)\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    y = np.ones((batch_size, 1))\n",
        "    discriminator.trainable = False\n",
        "    gan.train_on_batch(noise,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbueZVgKpvNO"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU\n",
        "import numpy as np\n",
        "\n",
        "# Load the SMILES dataset\n",
        "smiles_series = Chem_data['Smiles']\n",
        "\n",
        "# Tokenize the SMILES strings using the PubChem10M SMILES BPE tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "smiles_list = smiles_series.tolist()\n",
        "encoded_smiles = tokenizer(smiles_list, padding=True, truncation=True, max_length=100, return_tensors=\"pt\")\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuAS3DhLrIEU"
      },
      "outputs": [],
      "source": [
        "# Define the architecture of the generator and discriminator\n",
        "generator = Sequential()\n",
        "generator.add(Dense(128, input_dim=100))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(Dense(256))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(Dense(512))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Dense(512, input_dim=vocab_size))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dense(256))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4brnq37rPmU"
      },
      "outputs": [],
      "source": [
        "# Compile the GAN model\n",
        "gan_input = generator.input\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTcWEYgmuF4Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNPoe-BirSKm"
      },
      "outputs": [],
      "source": [
        "# Train the GAN on the training set\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the discriminator\n",
        "    real_samples = train_data.sample(batch_size)\n",
        "    real_X, real_y = real_samples.drop(['Smiles'], axis=1), np.ones((batch_size, 1))\n",
        "    fake_samples = generator.predict(np.random.normal(0, 1, (batch_size, 100)))\n",
        "    fake_X = np.zeros_like(real_X).astype('float32')\n",
        "    num_cols = min(fake_samples.shape[1], real_X.shape[1])\n",
        "    fake_X[:, :num_cols] = fake_samples[:, :num_cols]\n",
        "    fake_y = np.zeros((batch_size, 1))\n",
        "    X, y = np.vstack((real_X, fake_X)), np.vstack((real_y, fake_y))\n",
        "    discriminator.trainable = True\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    discriminator.train_on_batch(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAZb9iFIrU8F"
      },
      "outputs": [],
      "source": [
        "# Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    y = np.ones((batch_size, 1))\n",
        "    discriminator.trainable = False\n",
        "    gan.train_on_batch(noise,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkcFtYE8PmSU"
      },
      "source": [
        "Evalution Function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q5LPctqxitQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the SMILES dataset\n",
        "smiles_series = Chem_data['Smiles']\n",
        "smiles_list = smiles_series.tolist()\n",
        "\n",
        "# Tokenize the SMILES strings using the PubChem10M SMILES BPE tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "encoded_smiles = tokenizer(smiles_list, padding=True, truncation=True, max_length=100, return_tensors=\"pt\")\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "# Define the architecture of the generator and discriminator\n",
        "def make_generator():\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(128, input_dim=100))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(256))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(512))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(vocab_size, activation='softmax'))\n",
        "    return generator\n",
        "\n",
        "def make_discriminator():\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(512, input_dim=vocab_size))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    return discriminator\n",
        "\n",
        "generator = make_generator()\n",
        "discriminator = make_discriminator()\n",
        "\n",
        "# Compile the GAN model\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "gan = Sequential([generator, discriminator])\n",
        "discriminator.trainable = False\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train the GAN on the training set\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # Train the discriminator\n",
        "  real_samples = train_data['#Rotatable Bonds',\t'Passes Ro3'].sample(batch_size)\n",
        "  real_X, real_y = real_samples(axis=1), np.ones((batch_size, 1))\n",
        "  fake_samples = generator.predict(np.random.normal(0, 1, (batch_size, 100)))\n",
        "  fake_X = np.zeros_like(real_X).astype('float32')\n",
        "  num_cols = min(fake_samples.shape[1], real_X.shape[1])\n",
        "  fake_X[:, :num_cols] = fake_samples[:, :num_cols]\n",
        "  fake_y = np.zeros((batch_size, 1))\n",
        "  X, y = np.vstack((real_X, fake_X)), np.vstack((real_y, fake_y))\n",
        "  discriminator.trainable = True\n",
        "  discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "  X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "  y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "  discriminator.train_on_batch(X_tensor, y_tensor)\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPHwm7M0eKQK"
      },
      "outputs": [],
      "source": [
        "  # Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    y = np.ones((batch_size, 1))\n",
        "    discriminator.trainable = False\n",
        "    gan.train_on_batch(noise,y)\n",
        "\n",
        "  # Train the discriminator\n",
        "    real_samples = train_data.sample(batch_size)\n",
        "    real_X, real_y = real_samples.drop(['Smiles'], axis=1), np.ones((batch_size, 1))\n",
        "    fake_samples = generator.predict(np.random.normal(0, 1, (batch_size, 100)))\n",
        "    fake_X = np.zeros_like(real_X).astype('float64')\n",
        "    num_cols = min(fake_samples.shape[1], real_X.shape[1])\n",
        "    fake_X[:, :num_cols] = fake_samples[:, :num_cols]\n",
        "    fake_y = np.zeros((batch_size, 1))\n",
        "    X, y = np.vstack((real_X, fake_X)), np.vstack((real_y, fake_y))\n",
        "    discriminator.trainable = True\n",
        "    discriminator.train_on_batch(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zVcoqOx1B46"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Concatenate\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "\n",
        "\n",
        "# Tokenize the SMILES strings using the PubChem10M SMILES BPE tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "smiles_list = Chem_data['Smiles'].tolist()\n",
        "encoded_smiles = tokenizer(smiles_list, padding=True, truncation=True, max_length=100, return_tensors=\"pt\")\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "# Preprocess the properties\n",
        "\n",
        "properties = Chem_data.drop(['Smiles','Molecular Formula'], axis=1).values.astype('float32')\n",
        "num_properties = properties.shape[1]\n",
        "\n",
        "# Define the architecture of the generator and discriminator\n",
        "def make_generator():\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(128, input_dim=100+num_properties))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(256))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(512))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(vocab_size, activation='softmax'))\n",
        "    return generator\n",
        "\n",
        "def make_discriminator():\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(512, input_dim=vocab_size+num_properties))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    return discriminator\n",
        "\n",
        "generator = make_generator()\n",
        "discriminator = make_discriminator()\n",
        "\n",
        "# Compile the GAN model\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "gan = Sequential([generator, discriminator])\n",
        "discriminator.trainable = False\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train the GAN on the training set\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the discriminator\n",
        "    real_samples = Chem_data.sample(batch_size)\n",
        "    real_smiles, real_props = real_samples['Smiles'], real_samples.drop(['Smiles','Molecular Formula'], axis=1).values.astype('float32')\n",
        "    real_X, real_y = np.concatenate((encoded_smiles[real_smiles].numpy(), real_props), axis=1), np.ones((batch_size, 1))\n",
        "    fake_samples = generator.predict(np.hstack((np.random.normal(0, 1, (batch_size, 100)), real_props)))\n",
        "    fake_X, fake_y = np.concatenate((fake_samples, real_props), axis=1), np.zeros((batch_size, 1))\n",
        "    X, y = np.vstack((real_X, fake_X)), np.vstack((real_y, fake_y))\n",
        "    discriminator.trainable = True\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    discriminator.train_on_batch(X, y)\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    gan_X, gan_y = np.hstack((noise, real_props)), np.ones((batch_size, 1))\n",
        "    discriminator.trainable = False\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    gan.train_on_batch(gan_X, gan_y)\n",
        "\n",
        "    # Print the progress\n",
        "   \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, generator loss: {gen_loss:.4f}, discriminator loss: {disc_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBnPjZ2I3qeA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Concatenate, Input\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the SMILES strings using the PubChem10M SMILES BPE tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "smiles_list = Chem_data['Smiles'].tolist()\n",
        "encoded_smiles = tokenizer(smiles_list, padding=True, truncation=True, max_length=100, return_tensors=\"pt\")\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "# Preprocess the properties\n",
        "properties = Chem_data.drop(['Smiles','Molecular Formula'], axis=1).values.astype('float32')\n",
        "num_properties = properties.shape[1]\n",
        "\n",
        "# Define the architecture of the generator and discriminator\n",
        "def make_generator():\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(128, input_dim=100+num_properties))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(256))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(512))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(vocab_size, activation='softmax'))\n",
        "    return generator\n",
        "\n",
        "def make_discriminator():\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(512, input_dim=vocab_size+num_properties))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    return discriminator\n",
        "\n",
        "# Concatenate noise vector and properties\n",
        "z = Input(shape=(100,))\n",
        "prop = Input(shape=(num_properties,))\n",
        "combined_inputs = Concatenate()([z, prop])\n",
        "\n",
        "generator = make_generator()\n",
        "discriminator = make_discriminator()\n",
        "\n",
        "# Generate molecule from noise vector and properties\n",
        "molecule = generator(combined_inputs)\n",
        "\n",
        "# Compile the discriminator model\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Set discriminator to non-trainable\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Compile the GAN model\n",
        "gan = Model([z, prop], discriminator(molecule))\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train the GAN on the training set\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "latent_dim = 100\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the discriminator\n",
        "    real_samples = Chem_data.sample(batch_size)\n",
        "    real_smiles, real_props = real_samples['Smiles'], real_samples.drop(['Smiles'], axis=1).values.astype('float32')\n",
        "    real_X, real_y = np.concatenate((encoded_smiles[real_smiles].numpy(), real_props), axis=1), np.ones((batch_size, 1))\n",
        "    fake_samples = generator.predict([np.random.normal(0, 1, (batch_size, 100)), real_props])\n",
        "    fake_X, fake_y = np.concatenate((fake_samples, real_props), axis=1), np.zeros((batch_size, 1))\n",
        "    X, y = np.vstack((real_X, fake_X)), np.vstack((real_y, fake_y))\n",
        "    discriminator.trainable = True\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    discriminator.train_on_batch(X, y)\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    prop = real_props\n",
        "    gan_X = [noise, prop]\n",
        "    gan_y = np.ones((batch_size, 1))\n",
        "    discriminator.trainable = False\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    gan.train_on_batch(gan_X, gan_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSVAtTTy9oAC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Concatenate, Input\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "\n",
        "# Tokenize the SMILES strings using the PubChem10M SMILES BPE tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "smiles_list = Chem_data['Smiles'].tolist()\n",
        "encoded_smiles = tokenizer(smiles_list, padding=True, truncation=True, max_length=100, return_tensors=\"pt\")\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "# Preprocess the properties\n",
        "properties = Chem_data.drop(['Smiles'], axis=1).values.astype('float32')\n",
        "num_properties = properties.shape[1]\n",
        "\n",
        "# Define the architecture of the generator and discriminator\n",
        "def make_generator():\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(128, input_dim=100+num_properties))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(256))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(512))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Dense(vocab_size, activation='softmax'))\n",
        "    return generator\n",
        "\n",
        "def make_discriminator():\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(512, input_dim=vocab_size+num_properties))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    return discriminator\n",
        "\n",
        "# Concatenate noise vector and properties\n",
        "z = Input(shape=(100,))\n",
        "prop = Input(shape=(num_properties,))\n",
        "combined_inputs = Concatenate()([z, prop])\n",
        "\n",
        "generator = make_generator()\n",
        "discriminator = make_discriminator()\n",
        "\n",
        "# Generate molecule from noise vector and properties\n",
        "molecule = generator(combined_inputs)\n",
        "\n",
        "# Concatenate properties to generated molecule\n",
        "gen_input = Concatenate()([molecule, prop])\n",
        "\n",
        "# Compile the discriminator model\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Set discriminator to non-trainable\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Compile the GAN model\n",
        "gan = Model([z, prop], discriminator(gen_input))\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train the GAN model\n",
        "batch_size = 32\n",
        "num_epochs = 1000\n",
        "start_index = 0\n",
        "end_index = 31\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the discriminator\n",
        "    real_smiles = Chem_data['Smiles'][start_index:end_index]\n",
        "    #real_smiles = next(smiles_generator)\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_smiles = generator.predict([np.random.normal(size=(len(real_smiles), 100)), properties[:len(real_smiles)]])\n",
        "    #fake_smiles = generator.predict([np.random.normal(size=(batch_size, 100)), properties])\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "    discriminator.train_on_batch(np.concatenate([real_smiles, properties[:batch_size]], axis=1), real_labels)\n",
        "    discriminator.train_on_batch(np.concatenate([fake_smiles, properties[:batch_size]], axis=1), fake_labels)\n",
        "\n",
        "    # Train the generator\n",
        "    gan_labels = np.ones((batch_size, 1))\n",
        "    gan.train_on_batch([np.random.normal(size=(batch_size, 100)), properties], gan_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4kgP3C2ja_f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Concatenate, Input\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "\n",
        "# Tokenize the SMILES strings using the PubChem10M SMILES BPE tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "smiles_list = Chem_data['Smiles'].tolist()\n",
        "encoded_smiles = tokenizer(smiles_list, padding=True, truncation=True, max_length=100, return_tensors=\"pt\")\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "# Preprocess the properties\n",
        "properties = Chem_data.drop(['Smiles'], axis=1).values.astype('float64')\n",
        "num_properties = properties.shape[1]\n",
        "\n",
        "def make_generator():\n",
        "    z = Input(shape=(100,))\n",
        "    prop = Input(shape=(num_properties,))\n",
        "    x = Concatenate()([z, prop])\n",
        "    x = Dense(128)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(vocab_size, activation='softmax')(x)\n",
        "    generator = Model([z, prop], x)\n",
        "    return generator\n",
        "\n",
        "def make_discriminator():\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(512, input_dim=vocab_size+num_properties))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    return discriminator\n",
        "\n",
        "# Concatenate noise vector and properties\n",
        "z = Input(shape=(100,))\n",
        "prop = Input(shape=(num_properties,))\n",
        "\n",
        "generator = make_generator()\n",
        "molecule = generator([z, prop])\n",
        "\n",
        "discriminator = make_discriminator()\n",
        "gen_input = Concatenate()([molecule, prop])\n",
        "\n",
        "# Compile the discriminator model\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Set discriminator to non-trainable\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Compile the GAN model\n",
        "discriminator.trainable = False\n",
        "gan = Model([z, prop], discriminator(gen_input))\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "fake_smiles = generator.predict([np.random.normal(size=(len(real_smiles), 100)), properties[start_index:end_index]])\n",
        "\n",
        "# Train the GAN model\n",
        "batch_size = 32\n",
        "num_epochs = 1000\n",
        "start_index = 1\n",
        "end_index = 32\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the discriminator\n",
        "    \n",
        "    real_smiles = Chem_data['Smiles'][start_index:end_index].values.astype('str')\n",
        "    #real_smiles = next(smiles_generator)\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_smiles = generator.predict([np.random.normal(size=(len(real_smiles), 100)), properties[:len(real_smiles)]])\n",
        "    #fake_smiles = generator.predict([np.random.normal(size=(batch_size, 100)), properties])\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "    real_smiles = Chem_data['Smiles'][start_index:end_index].values.reshape(-1, 1)\n",
        "    #discriminator.train_on_batch(np.concatenate([real_smiles, properties[:len(real_smiles)]], axis=1), real_labels)\n",
        "    discriminator.train_on_batch(np.concatenate([real_smiles.reshape(-1, 1), properties[:len(real_smiles)]], axis=1), real_labels)\n",
        "\n",
        "\n",
        "    #discriminator.train_on_batch(np.concatenate([real_smiles.reshape(-1,1), properties[:batch_size]], axis=1), real_labels)\n",
        "\n",
        "    #discriminator.train_on_batch(np.concatenate([real_smiles, properties[:batch_size]], axis=1), real_labels)\n",
        "    discriminator.train_on_batch(np.concatenate([fake_smiles, properties[:batch_size]], axis=1), fake_labels)\n",
        "\n",
        "    # Train the generator\n",
        "    gan_labels = np.ones((batch_size, 1))\n",
        "    gan.train_on_batch([np.random.normal(size=(batch_size, 100)), properties], gan_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW8lXf5GmZHb"
      },
      "outputs": [],
      "source": [
        "real_smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCuYe4J2sCWo"
      },
      "outputs": [],
      "source": [
        "#Only smiles\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from rdkit import Chem\n",
        "from tensorflow.keras.layers import Dense, Input, LeakyReLU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load and preprocess the data\n",
        "smiles_list = Chem_data['Smiles'].tolist()\n",
        "\n",
        "# Tokenize the SMILES strings using the PubChem10M SMILES BPE tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "encoded_smiles = tokenizer(smiles_list, padding=True, truncation=True, max_length=100, return_tensors=\"pt\")\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "# Define the generator model\n",
        "def make_generator():\n",
        "    z = Input(shape=(100,))\n",
        "    x = Dense(128)(z)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(vocab_size, activation='softmax')(x)\n",
        "    generator = Model(z, x)\n",
        "    return generator\n",
        "\n",
        "# Define the discriminator model\n",
        "def make_discriminator():\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(512, input_dim=vocab_size))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    return discriminator\n",
        "\n",
        "# Concatenate noise vector and properties\n",
        "z = Input(shape=(100,))\n",
        "generator = make_generator()\n",
        "molecule = generator(z)\n",
        "\n",
        "discriminator = make_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Set discriminator to non-trainable\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Compile the GAN model\n",
        "gan_input = z\n",
        "gan_output = discriminator(molecule)\n",
        "gan = Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Train the GAN model\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the discriminator\n",
        "    real_smiles = np.array(smiles_list)\n",
        "    real_labels = np.ones((len(real_smiles), 1))\n",
        "    fake_smiles = generator.predict(np.random.normal(size=(len(real_smiles), 100)))\n",
        "    fake_labels = np.zeros((len(real_smiles), 1))\n",
        "    discriminator.train_on_batch(encoded_smiles.reshape(len(real_smiles), -1), real_labels)\n",
        "    discriminator.train_on_batch(fake_smiles.reshape(len(real_smiles), -1), fake_labels)\n",
        "\n",
        "    # Train the generator\n",
        "    gan_labels = np.ones((len(real_smiles), 1))\n",
        "    gan.train_on_batch(np.random.normal(size=(len(real_smiles), 100)), gan_labels)\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLiXxLsbesiq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "\n",
        "# Concatenate the input tensors\n",
        "concatenated = Concatenate()([input_tensor_1, input_tensor_2])\n",
        "\n",
        "# Pass the concatenated tensor through the sequential model\n",
        "output = sequential_model(concatenated)\n",
        "\n",
        "\n",
        "# Define the input layers for the generator and discriminator\n",
        "z = Input(shape=(100,))\n",
        "prop = Input(shape=(14,))\n",
        "\n",
        "# Generate molecule from noise vector and properties\n",
        "molecule = generator([z, prop])\n",
        "\n",
        "# Concatenate properties to generated molecule\n",
        "gen_input = Concatenate()([molecule, prop])\n",
        "\n",
        "# Compile the discriminator model\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Set discriminator to non-trainable\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Compile the GAN model\n",
        "gan = Model([z, prop], discriminator(gen_input))\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train the GAN model\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the discriminator\n",
        "    real_smiles = next(smiles_generator)\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_smiles = generator.predict([np.random.normal(size=(batch_size, 100)), properties])\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "    discriminator.train_on_batch(real_smiles, real_labels)\n",
        "    discriminator.train_on_batch(fake_smiles, fake_labels)\n",
        "\n",
        "    # Train the generator\n",
        "    gan_labels = np.ones((batch_size, 1))\n",
        "    gan.train_on_batch([np.random.normal(size=(batch_size, 100)), properties], gan_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hckFcSCK-Kvq"
      },
      "outputs": [],
      "source": [
        "# Print the summary of the GAN model\n",
        "gan.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9jrlpHQIl60"
      },
      "outputs": [],
      "source": [
        "pip install molgen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm4crWQsGvj9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from molgen.graph import GraphGenerator, MolGraph\n",
        "\n",
        "# Define a function to preprocess your dataset\n",
        "def preprocess_dataset(dataset_path):\n",
        "    # Read your dataset from a CSV file or any other format\n",
        "    # Here, we assume that the dataset has two columns: \"Smiles\" and \"Properties\"\n",
        "    # The \"Properties\" column should contain a vector of your 14 properties\n",
        "    # We also assume that the dataset has a header row\n",
        "\n",
        "    # Load the dataset into a list of MolGraphs\n",
        "    molgraphs = []\n",
        "    with open(dataset_path, \"r\") as f:\n",
        "        next(f)  # Skip the header row\n",
        "        for line in f:\n",
        "            smiles, props = line.strip().split(\",\")\n",
        "            props = np.array(props.split(), dtype=np.float32)\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is not None:\n",
        "                molgraph = MolGraph(mol)\n",
        "                molgraph.set_property(\"properties\", props)\n",
        "                molgraphs.append(molgraph)\n",
        "\n",
        "    return molgraphs\n",
        "\n",
        "# Preprocess your dataset\n",
        "\n",
        "molgraphs = preprocess_dataset(Chem_data)\n",
        "\n",
        "# Create a GraphGenerator object\n",
        "generator = GraphGenerator(num_nodes=9, latent_dim=32)\n",
        "\n",
        "# Train the generator on your dataset\n",
        "generator.fit(molgraphs, epochs=100, batch_size=32)\n",
        "\n",
        "# Generate new molecules based on specific properties\n",
        "props = np.array([0.2, 0.4, 0.1, 0.7, 0.9, 0.3, 0.5, 0.8, 0.6, 0.2, 0.4, 0.1, 0.7, 0.9], dtype=np.float32)\n",
        "generated_mol = generator.sample(props)\n",
        "generated_smiles = Chem.MolToSmiles(generated_mol)\n",
        "\n",
        "print(generated_smiles)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DrmcPqBxujR"
      },
      "source": [
        "## new try "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIIbWtoHSC1X"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load and preprocess data\n",
        "X = Chem_data['']\n",
        "y = dataset['smiles']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "# Normalize the input features\n",
        "X_train_normalized = (X_train - X_train.mean()) / X_train.std()\n",
        "X_test_normalized = (X_test - X_train.mean()) / X_train.std()\n",
        "\n",
        "# Train the model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_normalized, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = rf_model.predict(X_test_normalized)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean squared error: {mse}\")\n",
        "\n",
        "# Generate SMILES strings from new input descriptors\n",
        "new_descriptors = [[1.2, 3.4, ..., 0.5]]\n",
        "new_descriptors_normalized = (new_descriptors - X_train.mean()) / X_train.std()\n",
        "new_smiles = rf_model.predict(new_descriptors_normalized)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
